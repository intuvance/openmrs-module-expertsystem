<?xml version="1.0" encoding="UTF-8"?>
<!--
    This Source Code Form is subject to the terms of the Mozilla Public License,
    v. 2.0. If a copy of the MPL was not distributed with this file, You can
    obtain one at http://mozilla.org/MPL/2.0/. OpenMRS is also distributed under
    the terms of the Healthcare Disclaimer located at http://openmrs.org/license.

    Copyright (C) OpenMRS Inc. OpenMRS is a registered trademark and the OpenMRS
    graphic logo is a trademark of OpenMRS Inc.
-->

<module configVersion="1.2">
	
	<!-- Base Module Properties -->
	<id>${project.parent.artifactId}</id>
	<name>${project.parent.name}</name>
	<version>${project.parent.version}</version>
	<package>org.openmrs.module.expertsystem</package>
	<author>Christopher Miiro</author>
	<description>${project.parent.description}</description>
	<activator>${project.parent.groupId}.${project.parent.artifactId}.ExpertsystemActivator</activator>
	
	<require_version>${openmrsPlatformVersion}</require_version>
	
	<!-- Extensions -->
	<extension>
		<point>org.openmrs.admin.list</point>
		<class>org.openmrs.module.expertsystem.extension.html.AdminList</class>
	</extension>

	<require_modules>
		<require_module version="${webservices.restModuleVersion}">org.openmrs.module.webservices.rest</require_module>
	</require_modules>

	<aware_of_modules>
		<aware_of_module>org.openmrs.module.legacyui</aware_of_module>
		<aware_of_module>org.openmrs.module.emrapi</aware_of_module>
	</aware_of_modules>
	
	<messages>
		<lang>en</lang>
		<file>messages.properties</file>
	</messages>
	<messages>
		<lang>fr</lang>
		<file>messages_fr.properties</file>
	</messages>
	<messages>
		<lang>es</lang>
		<file>messages_es.properties</file>
	</messages>
	
	<globalProperty>
		<property>@MODULE_ID@.ollamaBaseUrl</property>
		<defaultValue>http://localhost:11434</defaultValue>
		<description>
			The network address where the Ollama server is running and accessible.
		</description>
		<datatype>java.lang.String</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.ollamaChatModel</property>
		<defaultValue>deepseek-r1:1.5b</defaultValue>
		<description>
			DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as Gemini 2.5 Pro.
		</description>
		<datatype>java.lang.String</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.ollamaReasoningModel</property>
		<defaultValue>deepseek-r1</defaultValue>
		<description>
			DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.
			The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic.
		</description>
		<datatype>java.lang.String</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.modelTemperature</property>
		<defaultValue>0.7</defaultValue>
		<description>
			In the context LLMs, temperature refers to a parameter that controls the randomness and creativity of the model's output.
			It essentially adjusts the probability distribution of the generated words, influencing how likely the model is to choose less predictable, more creative options.
		</description>
		<datatype>java.lang.Double</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.timeoutDuration</property>
		<defaultValue>10</defaultValue>
		<description>
			LangChain4j request timeout configuration
		</description>
		<datatype>java.lang.Long</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.enableThinking</property>
		<defaultValue>false</defaultValue>
		<description>
			To enable thinking in your Ollama integration with LangChain4j, you need to modify your configuration to set the enableThinking property to true
		</description>
		<datatype>java.lang.Boolean</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.enableStreaming</property>
		<defaultValue>true</defaultValue>
		<description>
			To enable streaming in your Ollama integration with LangChain4j, you need to modify your configuration to set the enableStreaming property to true
		</description>
		<datatype>java.lang.Boolean</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.corePoolSize</property>
		<defaultValue>2</defaultValue>
		<description>
			Core Threads: Threads to handle baseline AI processing workload
		</description>
		<datatype>java.lang.Integer</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.maximumPoolSize</property>
		<defaultValue>4</defaultValue>
		<description>
			Maximum Threads: Threads to handle peak loads without unbounded growth
		</description>
		<datatype>java.lang.Integer</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.keepAliveTime</property>
		<defaultValue>90</defaultValue>
		<description>
			Keep-Alive Time: Seconds to reclaim idle threads and conserve resources
		</description>
		<datatype>java.lang.Integer</datatype>
	</globalProperty>

	<globalProperty>
		<property>@MODULE_ID@.queueCapacity</property>
		<defaultValue>50</defaultValue>
		<description>
			Queue Capacity: Tasks to provide back-pressure and prevent memory explosion
		</description>
		<datatype>java.lang.Integer</datatype>
	</globalProperty>
</module>

